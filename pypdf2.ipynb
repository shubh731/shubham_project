{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83360c1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2 as pdf\n",
    "import nltk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a4713c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedReader name='B12800982S1119.pdf'>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"B12800982S1119.pdf\",\"rb\")\n",
    "file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2d2a4c13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PyPDF2._reader.PdfFileReader at 0x27aa1dc3790>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_reader = pdf.PdfFileReader(file)\n",
    "pdf_reader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea068f3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on PdfFileReader in module PyPDF2._reader object:\n",
      "\n",
      "class PdfFileReader(PdfReader)\n",
      " |  PdfFileReader(*args: Any, **kwargs: Any) -> None\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      PdfFileReader\n",
      " |      PdfReader\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *args: Any, **kwargs: Any) -> None\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from PdfReader:\n",
      " |  \n",
      " |  cacheGetIndirectObject(self, generation: int, idnum: int) -> Optional[PyPDF2.generic.PdfObject]\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :meth:`cache_get_indirect_object` instead.\n",
      " |  \n",
      " |  cacheIndirectObject(self, generation: int, idnum: int, obj: Optional[PyPDF2.generic.PdfObject]) -> Optional[PyPDF2.generic.PdfObject]\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :meth:`cache_indirect_object` instead.\n",
      " |  \n",
      " |  cache_get_indirect_object(self, generation: int, idnum: int) -> Optional[PyPDF2.generic.PdfObject]\n",
      " |  \n",
      " |  cache_indirect_object(self, generation: int, idnum: int, obj: Optional[PyPDF2.generic.PdfObject]) -> Optional[PyPDF2.generic.PdfObject]\n",
      " |  \n",
      " |  decode_permissions(self, permissions_code: int) -> Dict[str, bool]\n",
      " |  \n",
      " |  decrypt(self, password: Union[str, bytes]) -> PyPDF2._encryption.PasswordType\n",
      " |      When using an encrypted / secured PDF file with the PDF Standard\n",
      " |      encryption handler, this function will allow the file to be decrypted.\n",
      " |      It checks the given password against the document's user password and\n",
      " |      owner password, and then stores the resulting decryption key if either\n",
      " |      password is correct.\n",
      " |      \n",
      " |      It does not matter which password was matched.  Both passwords provide\n",
      " |      the correct decryption key that will allow the document to be used with\n",
      " |      this library.\n",
      " |      \n",
      " |      :param str password: The password to match.\n",
      " |      :return: `PasswordType`.\n",
      " |  \n",
      " |  getDestinationPageNumber(self, destination: PyPDF2.generic.Destination) -> int\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :meth:`get_destination_page_number` instead.\n",
      " |  \n",
      " |  getDocumentInfo(self) -> Optional[PyPDF2._reader.DocumentInformation]\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use the attribute :py:attr:`metadata` instead.\n",
      " |  \n",
      " |  getFields(self, tree: Optional[PyPDF2.generic.TreeObject] = None, retval: Optional[Dict[Any, Any]] = None, fileobj: Optional[Any] = None) -> Optional[Dict[str, Any]]\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :meth:`get_fields` instead.\n",
      " |  \n",
      " |  getFormTextFields(self) -> Dict[str, Any]\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :meth:`get_form_text_fields` instead.\n",
      " |  \n",
      " |  getIsEncrypted(self) -> bool\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :py:attr:`is_encrypted` instead.\n",
      " |  \n",
      " |  getNamedDestinations(self, tree: Optional[PyPDF2.generic.TreeObject] = None, retval: Optional[Any] = None) -> Dict[str, Any]\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :py:attr:`named_destinations` instead.\n",
      " |  \n",
      " |  getNumPages(self) -> int\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :code:`len(reader.pages)` instead.\n",
      " |  \n",
      " |  getObject(self, indirectReference: PyPDF2.generic.IndirectObject) -> Optional[PyPDF2.generic.PdfObject]\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :meth:`get_object` instead.\n",
      " |  \n",
      " |  getOutlines(self, node: Optional[PyPDF2.generic.DictionaryObject] = None, outline: Optional[Any] = None) -> List[Union[PyPDF2.generic.Destination, List[Union[PyPDF2.generic.Destination, List[PyPDF2.generic.Destination]]]]]\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :py:attr:`outline` instead.\n",
      " |  \n",
      " |  getPage(self, pageNumber: int) -> PyPDF2._page.PageObject\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :code:`reader.pages[pageNumber]` instead.\n",
      " |  \n",
      " |  getPageLayout(self) -> Optional[str]\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :py:attr:`page_layout` instead.\n",
      " |  \n",
      " |  getPageMode(self) -> Optional[Literal['/UseNone', '/UseOutlines', '/UseThumbs', '/FullScreen', '/UseOC', '/UseAttachments']]\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :py:attr:`page_mode` instead.\n",
      " |  \n",
      " |  getPageNumber(self, page: PyPDF2._page.PageObject) -> int\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :meth:`get_page_number` instead.\n",
      " |  \n",
      " |  getXmpMetadata(self) -> Optional[PyPDF2.xmp.XmpInformation]\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use the attribute :py:attr:`xmp_metadata` instead.\n",
      " |  \n",
      " |  get_destination_page_number(self, destination: PyPDF2.generic.Destination) -> int\n",
      " |      Retrieve page number of a given Destination object.\n",
      " |      \n",
      " |      :param Destination destination: The destination to get page number.\n",
      " |      :return: the page number or -1 if page not found\n",
      " |  \n",
      " |  get_fields(self, tree: Optional[PyPDF2.generic.TreeObject] = None, retval: Optional[Dict[Any, Any]] = None, fileobj: Optional[Any] = None) -> Optional[Dict[str, Any]]\n",
      " |      Extract field data if this PDF contains interactive form fields.\n",
      " |      \n",
      " |      The *tree* and *retval* parameters are for recursive use.\n",
      " |      \n",
      " |      :param fileobj: A file object (usually a text file) to write\n",
      " |          a report to on all interactive form fields found.\n",
      " |      :return: A dictionary where each key is a field name, and each\n",
      " |          value is a :class:`Field<PyPDF2.generic.Field>` object. By\n",
      " |          default, the mapping name is used for keys.\n",
      " |          ``None`` if form data could not be located.\n",
      " |  \n",
      " |  get_form_text_fields(self) -> Dict[str, Any]\n",
      " |      Retrieve form fields from the document with textual data.\n",
      " |      \n",
      " |      The key is the name of the form field, the value is the content of the\n",
      " |      field.\n",
      " |      \n",
      " |      If the document contains multiple form fields with the same name, the\n",
      " |      second and following will get the suffix _2, _3, ...\n",
      " |  \n",
      " |  get_object(self, indirect_reference: PyPDF2.generic.IndirectObject) -> Optional[PyPDF2.generic.PdfObject]\n",
      " |  \n",
      " |  get_page_number(self, page: PyPDF2._page.PageObject) -> int\n",
      " |      Retrieve page number of a given PageObject\n",
      " |      \n",
      " |      :param PageObject page: The page to get page number. Should be\n",
      " |          an instance of :class:`PageObject<PyPDF2._page.PageObject>`\n",
      " |      :return: the page number or -1 if page not found\n",
      " |  \n",
      " |  read(self, stream: Union[_io.BytesIO, _io.BufferedReader, _io.BufferedWriter, _io.FileIO]) -> None\n",
      " |  \n",
      " |  readNextEndLine(self, stream: Union[_io.BytesIO, _io.BufferedReader, _io.BufferedWriter, _io.FileIO], limit_offset: int = 0) -> bytes\n",
      " |      .. deprecated:: 1.28.0\n",
      " |  \n",
      " |  readObjectHeader(self, stream: Union[_io.BytesIO, _io.BufferedReader, _io.BufferedWriter, _io.FileIO]) -> Tuple[int, int]\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :meth:`read_object_header` instead.\n",
      " |  \n",
      " |  read_next_end_line(self, stream: Union[_io.BytesIO, _io.BufferedReader, _io.BufferedWriter, _io.FileIO], limit_offset: int = 0) -> bytes\n",
      " |      .. deprecated:: 2.1.0\n",
      " |  \n",
      " |  read_object_header(self, stream: Union[_io.BytesIO, _io.BufferedReader, _io.BufferedWriter, _io.FileIO]) -> Tuple[int, int]\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties inherited from PdfReader:\n",
      " |  \n",
      " |  documentInfo\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use the attribute :py:attr:`metadata` instead.\n",
      " |  \n",
      " |  isEncrypted\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :py:attr:`is_encrypted` instead.\n",
      " |  \n",
      " |  is_encrypted\n",
      " |      Read-only boolean property showing whether this PDF file is encrypted.\n",
      " |      Note that this property, if true, will remain true even after the\n",
      " |      :meth:`decrypt()<PyPDF2.PdfReader.decrypt>` method is called.\n",
      " |  \n",
      " |  metadata\n",
      " |      Retrieve the PDF file's document information dictionary, if it exists.\n",
      " |      Note that some PDF files use metadata streams instead of docinfo\n",
      " |      dictionaries, and these metadata streams will not be accessed by this\n",
      " |      function.\n",
      " |      \n",
      " |      :return: the document information of this PDF file\n",
      " |  \n",
      " |  namedDestinations\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :py:attr:`named_destinations` instead.\n",
      " |  \n",
      " |  named_destinations\n",
      " |      A read-only dictionary which maps names to\n",
      " |      :class:`Destinations<PyPDF2.generic.Destination>`\n",
      " |  \n",
      " |  numPages\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :code:`len(reader.pages)` instead.\n",
      " |  \n",
      " |  outline\n",
      " |      Read-only property for the outline (i.e., a collection of 'outline items'\n",
      " |      which are also known as 'bookmarks') present in the document.\n",
      " |      \n",
      " |      :return: a nested list of :class:`Destinations<PyPDF2.generic.Destination>`.\n",
      " |  \n",
      " |  outlines\n",
      " |      .. deprecated:: 2.9.0\n",
      " |      \n",
      " |          Use :py:attr:`outline` instead.\n",
      " |  \n",
      " |  pageLayout\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :py:attr:`page_layout` instead.\n",
      " |  \n",
      " |  pageMode\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use :py:attr:`page_mode` instead.\n",
      " |  \n",
      " |  page_layout\n",
      " |      Get the page layout.\n",
      " |      \n",
      " |      :return: Page layout currently being used.\n",
      " |      \n",
      " |      .. list-table:: Valid ``layout`` values\n",
      " |         :widths: 50 200\n",
      " |      \n",
      " |         * - /NoLayout\n",
      " |           - Layout explicitly not specified\n",
      " |         * - /SinglePage\n",
      " |           - Show one page at a time\n",
      " |         * - /OneColumn\n",
      " |           - Show one column at a time\n",
      " |         * - /TwoColumnLeft\n",
      " |           - Show pages in two columns, odd-numbered pages on the left\n",
      " |         * - /TwoColumnRight\n",
      " |           - Show pages in two columns, odd-numbered pages on the right\n",
      " |         * - /TwoPageLeft\n",
      " |           - Show two pages at a time, odd-numbered pages on the left\n",
      " |         * - /TwoPageRight\n",
      " |           - Show two pages at a time, odd-numbered pages on the right\n",
      " |  \n",
      " |  page_mode\n",
      " |      Get the page mode.\n",
      " |      \n",
      " |      :return: Page mode currently being used.\n",
      " |      \n",
      " |      .. list-table:: Valid ``mode`` values\n",
      " |         :widths: 50 200\n",
      " |      \n",
      " |         * - /UseNone\n",
      " |           - Do not show outline or thumbnails panels\n",
      " |         * - /UseOutlines\n",
      " |           - Show outline (aka bookmarks) panel\n",
      " |         * - /UseThumbs\n",
      " |           - Show page thumbnails panel\n",
      " |         * - /FullScreen\n",
      " |           - Fullscreen view\n",
      " |         * - /UseOC\n",
      " |           - Show Optional Content Group (OCG) panel\n",
      " |         * - /UseAttachments\n",
      " |           - Show attachments panel\n",
      " |  \n",
      " |  pages\n",
      " |      Read-only property that emulates a list of :py:class:`Page<PyPDF2._page.Page>` objects.\n",
      " |  \n",
      " |  pdf_header\n",
      " |  \n",
      " |  xfa\n",
      " |  \n",
      " |  xmpMetadata\n",
      " |      .. deprecated:: 1.28.0\n",
      " |      \n",
      " |          Use the attribute :py:attr:`xmp_metadata` instead.\n",
      " |  \n",
      " |  xmp_metadata\n",
      " |      XMP (Extensible Metadata Platform) data\n",
      " |      \n",
      " |      :return: a :class:`XmpInformation<xmp.XmpInformation>`\n",
      " |          instance that can be used to access XMP metadata from the document.\n",
      " |          or ``None`` if no metadata was found on the document root.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from PdfReader:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(pdf_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3d2f1df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Creator': 'Microsoft® Office Word 2007',\n",
       " '/CreationDate': 'D:20191029082121',\n",
       " '/ModDate': 'D:20191029082121',\n",
       " '/Producer': 'Microsoft® Office Word 2007'}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_reader. getDocumentInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfd5cb9c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_reader. getIsEncrypted() # its not encrepted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "584c7924",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_reader.getFormTextFields()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c76e312",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf_reader. getNumPages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b2e1693a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Type': '/Page',\n",
       " '/Parent': {'/Type': '/Pages',\n",
       "  '/Count': 4,\n",
       "  '/Kids': [IndirectObject(3, 0, 2725724829584),\n",
       "   IndirectObject(31, 0, 2725724829584),\n",
       "   IndirectObject(34, 0, 2725724829584),\n",
       "   IndirectObject(40, 0, 2725724829584)]},\n",
       " '/Resources': {'/XObject': {'/Image5': {'/Type': '/XObject',\n",
       "    '/Subtype': '/Image',\n",
       "    '/Width': 310,\n",
       "    '/Height': 237,\n",
       "    '/ColorSpace': '/DeviceRGB',\n",
       "    '/BitsPerComponent': 8,\n",
       "    '/Filter': '/DCTDecode',\n",
       "    '/Interpolate': True},\n",
       "   '/Image29': {'/Type': '/XObject',\n",
       "    '/Subtype': '/Image',\n",
       "    '/Width': 329,\n",
       "    '/Height': 357,\n",
       "    '/ColorSpace': '/DeviceRGB',\n",
       "    '/BitsPerComponent': 8,\n",
       "    '/Filter': '/DCTDecode',\n",
       "    '/Interpolate': True,\n",
       "    '/SMask': {'/Type': '/XObject',\n",
       "     '/Subtype': '/Image',\n",
       "     '/Width': 329,\n",
       "     '/Height': 357,\n",
       "     '/ColorSpace': '/DeviceGray',\n",
       "     '/Matte': [0, 0, 0],\n",
       "     '/BitsPerComponent': 8,\n",
       "     '/Interpolate': False,\n",
       "     '/Filter': '/FlateDecode'}}},\n",
       "  '/Font': {'/F1': {'/Type': '/Font',\n",
       "    '/Subtype': '/TrueType',\n",
       "    '/Name': '/F1',\n",
       "    '/BaseFont': '/Times#20New#20Roman,Bold',\n",
       "    '/Encoding': '/WinAnsiEncoding',\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/Times#20New#20Roman,Bold',\n",
       "     '/Flags': 32,\n",
       "     '/ItalicAngle': 0,\n",
       "     '/Ascent': 891,\n",
       "     '/Descent': -216,\n",
       "     '/CapHeight': 677,\n",
       "     '/AvgWidth': 427,\n",
       "     '/MaxWidth': 2558,\n",
       "     '/FontWeight': 700,\n",
       "     '/XHeight': 250,\n",
       "     '/Leading': 42,\n",
       "     '/StemV': 42,\n",
       "     '/FontBBox': [-558, -216, 2000, 677]},\n",
       "    '/FirstChar': 32,\n",
       "    '/LastChar': 121,\n",
       "    '/Widths': [250,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     333,\n",
       "     333,\n",
       "     0,\n",
       "     0,\n",
       "     250,\n",
       "     333,\n",
       "     250,\n",
       "     0,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     333,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     722,\n",
       "     667,\n",
       "     722,\n",
       "     722,\n",
       "     667,\n",
       "     611,\n",
       "     778,\n",
       "     778,\n",
       "     389,\n",
       "     500,\n",
       "     778,\n",
       "     667,\n",
       "     944,\n",
       "     722,\n",
       "     778,\n",
       "     611,\n",
       "     0,\n",
       "     722,\n",
       "     556,\n",
       "     667,\n",
       "     722,\n",
       "     722,\n",
       "     1000,\n",
       "     722,\n",
       "     722,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     500,\n",
       "     556,\n",
       "     444,\n",
       "     556,\n",
       "     444,\n",
       "     333,\n",
       "     500,\n",
       "     556,\n",
       "     278,\n",
       "     0,\n",
       "     0,\n",
       "     278,\n",
       "     833,\n",
       "     556,\n",
       "     500,\n",
       "     556,\n",
       "     556,\n",
       "     444,\n",
       "     389,\n",
       "     333,\n",
       "     556,\n",
       "     500,\n",
       "     0,\n",
       "     0,\n",
       "     500]},\n",
       "   '/F2': {'/Type': '/Font',\n",
       "    '/Subtype': '/TrueType',\n",
       "    '/Name': '/F2',\n",
       "    '/BaseFont': '/Times#20New#20Roman',\n",
       "    '/Encoding': '/WinAnsiEncoding',\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/Times#20New#20Roman',\n",
       "     '/Flags': 32,\n",
       "     '/ItalicAngle': 0,\n",
       "     '/Ascent': 891,\n",
       "     '/Descent': -216,\n",
       "     '/CapHeight': 693,\n",
       "     '/AvgWidth': 401,\n",
       "     '/MaxWidth': 2568,\n",
       "     '/FontWeight': 400,\n",
       "     '/XHeight': 250,\n",
       "     '/Leading': 42,\n",
       "     '/StemV': 40,\n",
       "     '/FontBBox': [-568, -216, 2000, 693]},\n",
       "    '/FirstChar': 32,\n",
       "    '/LastChar': 239,\n",
       "    '/Widths': [250,\n",
       "     0,\n",
       "     408,\n",
       "     0,\n",
       "     0,\n",
       "     833,\n",
       "     778,\n",
       "     180,\n",
       "     333,\n",
       "     333,\n",
       "     0,\n",
       "     0,\n",
       "     250,\n",
       "     333,\n",
       "     250,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     278,\n",
       "     278,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     921,\n",
       "     722,\n",
       "     667,\n",
       "     667,\n",
       "     722,\n",
       "     611,\n",
       "     556,\n",
       "     722,\n",
       "     0,\n",
       "     333,\n",
       "     0,\n",
       "     722,\n",
       "     611,\n",
       "     889,\n",
       "     722,\n",
       "     722,\n",
       "     556,\n",
       "     0,\n",
       "     667,\n",
       "     556,\n",
       "     611,\n",
       "     722,\n",
       "     722,\n",
       "     944,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     444,\n",
       "     500,\n",
       "     444,\n",
       "     500,\n",
       "     444,\n",
       "     333,\n",
       "     500,\n",
       "     500,\n",
       "     278,\n",
       "     278,\n",
       "     500,\n",
       "     278,\n",
       "     778,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     333,\n",
       "     389,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     722,\n",
       "     500,\n",
       "     500,\n",
       "     444,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     278]},\n",
       "   '/F3': {'/Type': '/Font',\n",
       "    '/Subtype': '/TrueType',\n",
       "    '/Name': '/F3',\n",
       "    '/BaseFont': '/Times#20New#20Roman,Italic',\n",
       "    '/Encoding': '/WinAnsiEncoding',\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/Times#20New#20Roman,Italic',\n",
       "     '/Flags': 32,\n",
       "     '/ItalicAngle': -16.4,\n",
       "     '/Ascent': 891,\n",
       "     '/Descent': -216,\n",
       "     '/CapHeight': 694,\n",
       "     '/AvgWidth': 402,\n",
       "     '/MaxWidth': 1831,\n",
       "     '/FontWeight': 400,\n",
       "     '/XHeight': 250,\n",
       "     '/Leading': 42,\n",
       "     '/StemV': 40,\n",
       "     '/FontBBox': [-498, -216, 1333, 694]},\n",
       "    '/FirstChar': 32,\n",
       "    '/LastChar': 239,\n",
       "    '/Widths': [250,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     778,\n",
       "     0,\n",
       "     333,\n",
       "     333,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     333,\n",
       "     250,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     333,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     611,\n",
       "     611,\n",
       "     667,\n",
       "     722,\n",
       "     611,\n",
       "     611,\n",
       "     722,\n",
       "     722,\n",
       "     333,\n",
       "     0,\n",
       "     667,\n",
       "     556,\n",
       "     833,\n",
       "     667,\n",
       "     722,\n",
       "     611,\n",
       "     0,\n",
       "     611,\n",
       "     500,\n",
       "     556,\n",
       "     0,\n",
       "     611,\n",
       "     833,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     500,\n",
       "     500,\n",
       "     444,\n",
       "     500,\n",
       "     444,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     278,\n",
       "     278,\n",
       "     444,\n",
       "     278,\n",
       "     722,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     0,\n",
       "     389,\n",
       "     389,\n",
       "     278,\n",
       "     500,\n",
       "     444,\n",
       "     0,\n",
       "     0,\n",
       "     444,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     760,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     278]},\n",
       "   '/F4': {'/Type': '/Font',\n",
       "    '/Subtype': '/Type0',\n",
       "    '/BaseFont': '/Symbol',\n",
       "    '/Encoding': '/Identity-H',\n",
       "    '/DescendantFonts': [IndirectObject(14, 0, 2725724829584)],\n",
       "    '/ToUnicode': {'/Filter': '/FlateDecode'}},\n",
       "   '/F5': {'/Type': '/Font',\n",
       "    '/Subtype': '/TrueType',\n",
       "    '/Name': '/F5',\n",
       "    '/BaseFont': '/Times#20New#20Roman,BoldItalic',\n",
       "    '/Encoding': '/WinAnsiEncoding',\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/Times#20New#20Roman,BoldItalic',\n",
       "     '/Flags': 32,\n",
       "     '/ItalicAngle': -16.4,\n",
       "     '/Ascent': 891,\n",
       "     '/Descent': -216,\n",
       "     '/CapHeight': 677,\n",
       "     '/AvgWidth': 412,\n",
       "     '/MaxWidth': 1948,\n",
       "     '/FontWeight': 700,\n",
       "     '/XHeight': 250,\n",
       "     '/Leading': 42,\n",
       "     '/StemV': 41,\n",
       "     '/FontBBox': [-547, -216, 1401, 677]},\n",
       "    '/FirstChar': 32,\n",
       "    '/LastChar': 122,\n",
       "    '/Widths': [250,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     833,\n",
       "     0,\n",
       "     0,\n",
       "     333,\n",
       "     333,\n",
       "     0,\n",
       "     0,\n",
       "     250,\n",
       "     333,\n",
       "     250,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     500,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     500,\n",
       "     500,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     667,\n",
       "     667,\n",
       "     667,\n",
       "     722,\n",
       "     0,\n",
       "     667,\n",
       "     722,\n",
       "     0,\n",
       "     389,\n",
       "     0,\n",
       "     667,\n",
       "     611,\n",
       "     889,\n",
       "     722,\n",
       "     0,\n",
       "     611,\n",
       "     0,\n",
       "     667,\n",
       "     556,\n",
       "     611,\n",
       "     0,\n",
       "     667,\n",
       "     889,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     500,\n",
       "     500,\n",
       "     444,\n",
       "     500,\n",
       "     444,\n",
       "     333,\n",
       "     500,\n",
       "     556,\n",
       "     278,\n",
       "     278,\n",
       "     500,\n",
       "     278,\n",
       "     778,\n",
       "     556,\n",
       "     500,\n",
       "     500,\n",
       "     0,\n",
       "     389,\n",
       "     389,\n",
       "     278,\n",
       "     556,\n",
       "     444,\n",
       "     667,\n",
       "     500,\n",
       "     444,\n",
       "     389]},\n",
       "   '/F6': {'/Type': '/Font',\n",
       "    '/Subtype': '/Type0',\n",
       "    '/BaseFont': '/Times#20New#20Roman,BoldItalic',\n",
       "    '/Encoding': '/Identity-H',\n",
       "    '/DescendantFonts': [IndirectObject(21, 0, 2725724829584)],\n",
       "    '/ToUnicode': {'/Filter': '/FlateDecode'}},\n",
       "   '/F7': {'/Type': '/Font',\n",
       "    '/Subtype': '/Type0',\n",
       "    '/BaseFont': '/Times#20New#20Roman',\n",
       "    '/Encoding': '/Identity-H',\n",
       "    '/DescendantFonts': [IndirectObject(26, 0, 2725724829584)],\n",
       "    '/ToUnicode': {'/Filter': '/FlateDecode'}}},\n",
       "  '/ProcSet': ['/PDF', '/Text', '/ImageB', '/ImageC', '/ImageI']},\n",
       " '/MediaBox': [0, 0, 595.32, 841.92],\n",
       " '/Contents': {'/Filter': '/FlateDecode'},\n",
       " '/Group': {'/Type': '/Group', '/S': '/Transparency', '/CS': '/DeviceRGB'},\n",
       " '/Tabs': '/S',\n",
       " '/StructParents': 0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page1 = pdf_reader.getPage(0)\n",
    "page1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d2f9d58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'International Journal of Recent Technology and Engineering (IJRTE)  \\nISSN: 2277 -3878, Volume -8, Issue -2S11, September 2019  \\n2423  Published By:  \\nBlue Eyes Intelligence Engineering \\n& Sciences Publication  Retrieval Number: B1280 0982S1119/2019©BEIESP  \\nDOI: 10.35940/ijrte.B1280 .0982S1119  \\n \\n \\uf020 \\nAbstract —Natural Language Processing is a vital field of \\nresearch having applications in different subjects. Text \\nClassification is a part of NLP where the text is converted into a \\nmachine -readable form by performing various methods. \\nTokenizing, part -of-speech tagging, stemming, chunking are some \\nof the text classification methods. Implementing these methods on \\nour data gives us a classified data on which we will train the model \\nto detect spam and ham messages using Scikit -Learn Classifiers. \\nWe proposed a m odel to solve the issue of classifying messages as \\nspam or ham by experimenting and analyzing the relative \\nstrengths of several machine learning algorithms such as \\nK-Nearest Neighbors (KNN), Decision Tree Classifier, Random \\nForest Classifier, Logistic Regr ession, SGD Classifier, \\nMultinomial Naive Bayes(NB), Support Vector Machine(SVM) to \\nhave a logical comparison of the performance measures of the \\nmethods we utilized in this research. The algorithm we proposed \\nachieved an average accuracy of 98.49% with SVM  model on \\n‘SMS Spam Collection’ dataset . \\nI. INTRODUCTION  \\nWe get hundreds of messages from unknown sources and \\nour inbox is filled with unwanted emails. These unwanted \\nmessages are called spam and essential messages are called \\nham mails. We will prepare a m odel that will categorize \\nmessages in mobile devices as spam or ham. In order to \\nachieve this, data from the messages is to be collected first \\nand natural language processing techniques are to be applied \\non it.  \\nThe spam filtering among messages helps the m obile user \\nto have a good visualization of the inbox. Unnecessary \\nmessages will be marked as spam so users need not waste \\ntheir time reading them. In this paper, we propose to classify \\ndata in the messages as either spam (unwanted) or \\nham(wanted) messages.  We devised our own spam detector.  \\nII. RELATED WORK  \\nIdentifying spam messages from inbox has been done by \\nvarious methodologies. Below are some of the approaches.  \\n1. Sethi, P., Bhandari, V., &Kohli, B. (2017). “SMS spam \\ndetection and comparison of various machine learning \\nalgorithms.” 2017 International Conference on Computing \\nand Communication Technologies for Smart Nation \\n(IC3TSN).  \\n2. DelviaArifin, D., Shaufiah, &Bijaksana, M. A. \\n(2016).“Enhancing spam detection on mobile phone Short \\nMessage Service (SMS)  performance using FP -growth and \\nNaive Bayes Classifier.”2016 IEEE Asia Pacific Conference \\non Wireless and Mobile (APWiMob).  \\n3. Agarwal, S., Kaur, S., &Garhwal, S. (2015). “SMS spam \\ndetection for Indian messages.” 2015 1st International \\nConference on Next Generation Computing Technologies  \\n \\nRevised Versi on Manuscript Received on 10 September, 2019 . \\nBollamPragna , (email: bpragna98@gmail.com)  \\nDr.M.RamaBai (Professor), (email: rama@mgit.ac.in)   \\n(NGCT).  \\n4.   Gupta, M., Bakliwal, A., Agarwal, S., &Mehndiratta, \\nP. (2018). “A Comparative Study of Spam SMS Detection \\nUsing Machine Learning Classifiers.” 2018 Eleventh \\nInternational Conference on Contemporary Computing  \\n(IC3).  \\nIII. ALGORITHM  \\nSpam is unsolicited bulk messages that are not required for \\nthe users but are forced into their inbox. Spams are sent \\nmostly by advertisers, tricksters or by fraud people. \\nUnderstanding if a message is spam or not can be easily done  \\nby reading it once. Our purpose is to detect spam by using \\nvarious algorithms and measuring their accuracy to find the \\nbest fitting algorithm. The common classical approaches \\nwhich use white -lists and black -lists methods do not work \\nproperly as they are o nly capable of blocking an entire server \\n(source) from sending messages, that may contain some \\nimportant messages (false positives). Therefore, spam \\nfiltration is to be done using text classification techniques.  \\nIn our experiments we performed text pre -pro cessing in \\nthe first part which gives the annotated data that is split into \\ntwo parts called training set and testing set to check the \\naccuracy. The below figure represents the flow of our \\ntraining and testing on the data using various algorithms.  \\n \\n \\nVarious types of spam filters used are given below.  \\n1. Blatant Blocking -  \\n  \\nSpam Detection using NLP Techniques  \\nBollamPragna, M.RamaBai  '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_page1 = page1.extract_text(0)\n",
    "F_page1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084305c3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc89bb35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Type': '/Page',\n",
       " '/Parent': {'/Type': '/Pages',\n",
       "  '/Count': 4,\n",
       "  '/Kids': [IndirectObject(3, 0, 2725724829584),\n",
       "   IndirectObject(31, 0, 2725724829584),\n",
       "   IndirectObject(34, 0, 2725724829584),\n",
       "   IndirectObject(40, 0, 2725724829584)]},\n",
       " '/Resources': {'/XObject': {'/Image33': {'/Type': '/XObject',\n",
       "    '/Subtype': '/Image',\n",
       "    '/Width': 310,\n",
       "    '/Height': 237,\n",
       "    '/ColorSpace': '/DeviceRGB',\n",
       "    '/BitsPerComponent': 8,\n",
       "    '/Filter': '/DCTDecode',\n",
       "    '/Interpolate': True}},\n",
       "  '/Font': {'/F1': {'/Type': '/Font',\n",
       "    '/Subtype': '/TrueType',\n",
       "    '/Name': '/F1',\n",
       "    '/BaseFont': '/Times#20New#20Roman,Bold',\n",
       "    '/Encoding': '/WinAnsiEncoding',\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/Times#20New#20Roman,Bold',\n",
       "     '/Flags': 32,\n",
       "     '/ItalicAngle': 0,\n",
       "     '/Ascent': 891,\n",
       "     '/Descent': -216,\n",
       "     '/CapHeight': 677,\n",
       "     '/AvgWidth': 427,\n",
       "     '/MaxWidth': 2558,\n",
       "     '/FontWeight': 700,\n",
       "     '/XHeight': 250,\n",
       "     '/Leading': 42,\n",
       "     '/StemV': 42,\n",
       "     '/FontBBox': [-558, -216, 2000, 677]},\n",
       "    '/FirstChar': 32,\n",
       "    '/LastChar': 121,\n",
       "    '/Widths': [250,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     333,\n",
       "     333,\n",
       "     0,\n",
       "     0,\n",
       "     250,\n",
       "     333,\n",
       "     250,\n",
       "     0,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     333,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     722,\n",
       "     667,\n",
       "     722,\n",
       "     722,\n",
       "     667,\n",
       "     611,\n",
       "     778,\n",
       "     778,\n",
       "     389,\n",
       "     500,\n",
       "     778,\n",
       "     667,\n",
       "     944,\n",
       "     722,\n",
       "     778,\n",
       "     611,\n",
       "     0,\n",
       "     722,\n",
       "     556,\n",
       "     667,\n",
       "     722,\n",
       "     722,\n",
       "     1000,\n",
       "     722,\n",
       "     722,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     500,\n",
       "     556,\n",
       "     444,\n",
       "     556,\n",
       "     444,\n",
       "     333,\n",
       "     500,\n",
       "     556,\n",
       "     278,\n",
       "     0,\n",
       "     0,\n",
       "     278,\n",
       "     833,\n",
       "     556,\n",
       "     500,\n",
       "     556,\n",
       "     556,\n",
       "     444,\n",
       "     389,\n",
       "     333,\n",
       "     556,\n",
       "     500,\n",
       "     0,\n",
       "     0,\n",
       "     500]},\n",
       "   '/F2': {'/Type': '/Font',\n",
       "    '/Subtype': '/TrueType',\n",
       "    '/Name': '/F2',\n",
       "    '/BaseFont': '/Times#20New#20Roman',\n",
       "    '/Encoding': '/WinAnsiEncoding',\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/Times#20New#20Roman',\n",
       "     '/Flags': 32,\n",
       "     '/ItalicAngle': 0,\n",
       "     '/Ascent': 891,\n",
       "     '/Descent': -216,\n",
       "     '/CapHeight': 693,\n",
       "     '/AvgWidth': 401,\n",
       "     '/MaxWidth': 2568,\n",
       "     '/FontWeight': 400,\n",
       "     '/XHeight': 250,\n",
       "     '/Leading': 42,\n",
       "     '/StemV': 40,\n",
       "     '/FontBBox': [-568, -216, 2000, 693]},\n",
       "    '/FirstChar': 32,\n",
       "    '/LastChar': 239,\n",
       "    '/Widths': [250,\n",
       "     0,\n",
       "     408,\n",
       "     0,\n",
       "     0,\n",
       "     833,\n",
       "     778,\n",
       "     180,\n",
       "     333,\n",
       "     333,\n",
       "     0,\n",
       "     0,\n",
       "     250,\n",
       "     333,\n",
       "     250,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     278,\n",
       "     278,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     921,\n",
       "     722,\n",
       "     667,\n",
       "     667,\n",
       "     722,\n",
       "     611,\n",
       "     556,\n",
       "     722,\n",
       "     0,\n",
       "     333,\n",
       "     0,\n",
       "     722,\n",
       "     611,\n",
       "     889,\n",
       "     722,\n",
       "     722,\n",
       "     556,\n",
       "     0,\n",
       "     667,\n",
       "     556,\n",
       "     611,\n",
       "     722,\n",
       "     722,\n",
       "     944,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     444,\n",
       "     500,\n",
       "     444,\n",
       "     500,\n",
       "     444,\n",
       "     333,\n",
       "     500,\n",
       "     500,\n",
       "     278,\n",
       "     278,\n",
       "     500,\n",
       "     278,\n",
       "     778,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     333,\n",
       "     389,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     722,\n",
       "     500,\n",
       "     500,\n",
       "     444,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     278]},\n",
       "   '/F3': {'/Type': '/Font',\n",
       "    '/Subtype': '/TrueType',\n",
       "    '/Name': '/F3',\n",
       "    '/BaseFont': '/Times#20New#20Roman,Italic',\n",
       "    '/Encoding': '/WinAnsiEncoding',\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/Times#20New#20Roman,Italic',\n",
       "     '/Flags': 32,\n",
       "     '/ItalicAngle': -16.4,\n",
       "     '/Ascent': 891,\n",
       "     '/Descent': -216,\n",
       "     '/CapHeight': 694,\n",
       "     '/AvgWidth': 402,\n",
       "     '/MaxWidth': 1831,\n",
       "     '/FontWeight': 400,\n",
       "     '/XHeight': 250,\n",
       "     '/Leading': 42,\n",
       "     '/StemV': 40,\n",
       "     '/FontBBox': [-498, -216, 1333, 694]},\n",
       "    '/FirstChar': 32,\n",
       "    '/LastChar': 239,\n",
       "    '/Widths': [250,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     778,\n",
       "     0,\n",
       "     333,\n",
       "     333,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     333,\n",
       "     250,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     333,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     611,\n",
       "     611,\n",
       "     667,\n",
       "     722,\n",
       "     611,\n",
       "     611,\n",
       "     722,\n",
       "     722,\n",
       "     333,\n",
       "     0,\n",
       "     667,\n",
       "     556,\n",
       "     833,\n",
       "     667,\n",
       "     722,\n",
       "     611,\n",
       "     0,\n",
       "     611,\n",
       "     500,\n",
       "     556,\n",
       "     0,\n",
       "     611,\n",
       "     833,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     500,\n",
       "     500,\n",
       "     444,\n",
       "     500,\n",
       "     444,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     278,\n",
       "     278,\n",
       "     444,\n",
       "     278,\n",
       "     722,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     0,\n",
       "     389,\n",
       "     389,\n",
       "     278,\n",
       "     500,\n",
       "     444,\n",
       "     0,\n",
       "     0,\n",
       "     444,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     760,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     278]},\n",
       "   '/F7': {'/Type': '/Font',\n",
       "    '/Subtype': '/Type0',\n",
       "    '/BaseFont': '/Times#20New#20Roman',\n",
       "    '/Encoding': '/Identity-H',\n",
       "    '/DescendantFonts': [IndirectObject(26, 0, 2725724829584)],\n",
       "    '/ToUnicode': {'/Filter': '/FlateDecode'}}},\n",
       "  '/ProcSet': ['/PDF', '/Text', '/ImageB', '/ImageC', '/ImageI']},\n",
       " '/MediaBox': [0, 0, 595.32, 841.92],\n",
       " '/Contents': {'/Filter': '/FlateDecode'},\n",
       " '/Group': {'/Type': '/Group', '/S': '/Transparency', '/CS': '/DeviceRGB'},\n",
       " '/Tabs': '/S',\n",
       " '/StructParents': 1}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page2 = pdf_reader.getPage(1)\n",
    "page2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e424d4fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nSpam Detection using NLP Techniques  \\n2424  Published By:  \\nBlue Eyes Intelligence Engineering \\n& Sciences Publication  Retrieval Number: B1280 0982S1119/2019©BEIESP  \\nDOI: 10.35940/ijrte.B1280 .0982S1119  \\n \\n Emails are deleted even before they reach the inbox. This \\nblocking is done blatantly.  \\n2. Bulk Email Filter -  \\nThis filters out those emails which are passed on through \\nother catego ries but are unnecessary or spam messages.  \\n3. Category Filters -  \\nAccording to the specific content like email addresses etc, \\na user is allowed to define their own rule to enable the \\nfiltering of the messages. There can be one or more \\nuser -defined rules.  \\n4. Null Sender Disposition -  \\nMessages are disposed if they do not have an SMTP \\nenvelope sender address.  \\n5. Null Sender Header Tag Validation -  \\nAll the messages are validated by checking security digital \\nsignature of each message in the inbox.  \\nIV. FRAMEWORK  \\nVarious tools, techniques and data set used in this work is \\ndescribed in this section. As cellular messages repeatedly \\nhave a number of acronyms, efficiency of the filters is \\naffected by it. So a large and valid message dataset is used in \\nthis process.  \\n4.1  Tools and Algorithms  \\nThe machine learning algorithms used in the work are \\ndescribed in detail in this section.  \\n4.1.1 Naïve Bayes (NB)  \\nNaive Bayes Classiﬁer uses Bayes Theorem, which \\ndetermines the occurrence probability of an event \\nconsidering the probability of an occurred event. Linearly \\nseparable problems are solved extremely well by Naïve  \\nBayes classiﬁer and for non -linearly separab le questions, it \\nperforms reasonably good.  \\nMultinomial Naive Bayes classifier uses a multinomial \\ndistribution for each one of the features generated on data. \\nThis is a particular instance of a Naive Bayes classifier.  \\n4.1.2 Stochastic Gradient Descent  \\nAn o bjective function is optimized iteratively with suitable \\nsmoothness properties (e.g. subdifferentiable or \\ndifferentiable) in Stochastic Gradient Descent Algorithm \\n(SGD).  \\n4.1.3 Support Vector Machine (SVM)  \\nThe SVM classiﬁer creates an N -dimensional hyperpl ane \\nwhich divides the data into two categories. SVM models are \\nsimilar to a Neural Network. SVM classifier usually takes the \\ninput data and for every input taken it outputs the class to \\nwhich this input belongs. Two class problems are solved by \\nSVM which i s a non -probabilistic binary linear classiﬁer.  \\n4.1.4 Logistic Regression  \\nAlthough many sophisticated statistical models exit, \\nLogistic regression( or logit regression) uses a logistic \\nfunction to model a binary dependent variable. In regression \\nanalysis, i t estimates the parameters of a logit model which is \\nin the form of binary regression. In mathematical words a binary logistic model has a dependent variable with two \\npossible outcomes. These outcomes can be labelled as “0” \\nand “1” which usually represent two opposite classes such as \\npass/fail, win/loose.  \\n4.1.5  K-Nearest Neighbours  \\nIn pattern recognition, the k -nearest neighbors algorithm \\n(k-NN) is a non -parametric method used for classification \\nand regression. In both cases, the input is same but the outpu t \\nvaries. Input contains the  closest k training examples in the \\nfeature space whereas, the output is decided depending on the \\nk-NN usage for classification or regression:  \\n1. The output of a k -NN classifier has a class membership. \\nClassification of object s is done by a plurality vote of its \\nneighbours and the object is assigned to the class that is most \\ncommon among its k nearest neighbours. When „k‟ has a \\nvalue one, the object is directly given to the class which has a \\nsingle nearest neighbour.  \\n2. In k -NN  regression, property value gives the output for \\nthe object. This value is the average of the values of k nearest \\nneighbours.  \\n \\n \\n4.1.6  Random Forest Classifier  \\nEnsemble learning method for classification, regression \\nand other tasks is Random forests(or rand om decision \\nforests) which operate at training time by constructing a \\nmultitude of decision trees and giving the class as output \\nwhich is the mean prediction of the individual trees or mode \\nof the classes.  \\n4.1.7 Decision Tree Classifier  \\nA decision tree has  a flowchart -like structure containing \\nnodes. Each internal node is a \"test\" on an attribute which \\nbranches into two nodes. These two nodes are the outputs of \\nthe decision in the test. The tree ends with leaf nodes which \\nrepresent a class label (decision t aken after computing all \\nattributes). The path from the root node to reach one leaf node \\nis a single classification rule. Three types of nodes are in a \\ndecision tree which are given below.  \\n \\n1. Decision nodes – generally given by squares  \\n2. Chance nodes – generally given by circles  \\n3. End nodes – generally given by triangles.  \\n4.1.8 Ensemble Methods  \\nThe meta -algorithms combine various machine learning \\nmethods into one model which predicts the output and either \\nreduces the variance (bagging), bias (boosting),  or enhance \\npredictions (stacking). We used the voting classifier in our \\nexperiment.  \\nVoting Classifier  \\nThe Ensemble Vote Classifier is a meta -classifier that \\ncombines machine learning classifiers for classification by  \\nreferring  to majority or plurality voting which are either \\nsimilar or conceptually different. (For simplicity, we will \\nrefer to both majority and plurality voting as majority  \\n  '"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_page2 = page2.extract_text(0)\n",
    "F_page2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ee5cee8f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'/Type': '/Page',\n",
       " '/Parent': {'/Type': '/Pages',\n",
       "  '/Count': 4,\n",
       "  '/Kids': [IndirectObject(3, 0, 2725724829584),\n",
       "   IndirectObject(31, 0, 2725724829584),\n",
       "   IndirectObject(34, 0, 2725724829584),\n",
       "   IndirectObject(40, 0, 2725724829584)]},\n",
       " '/Resources': {'/XObject': {'/Image33': {'/Type': '/XObject',\n",
       "    '/Subtype': '/Image',\n",
       "    '/Width': 310,\n",
       "    '/Height': 237,\n",
       "    '/ColorSpace': '/DeviceRGB',\n",
       "    '/BitsPerComponent': 8,\n",
       "    '/Filter': '/DCTDecode',\n",
       "    '/Interpolate': True},\n",
       "   '/Image36': {'/Type': '/XObject',\n",
       "    '/Subtype': '/Image',\n",
       "    '/Width': 329,\n",
       "    '/Height': 302,\n",
       "    '/ColorSpace': '/DeviceRGB',\n",
       "    '/BitsPerComponent': 8,\n",
       "    '/Filter': '/DCTDecode',\n",
       "    '/Interpolate': True,\n",
       "    '/SMask': {'/Type': '/XObject',\n",
       "     '/Subtype': '/Image',\n",
       "     '/Width': 329,\n",
       "     '/Height': 302,\n",
       "     '/ColorSpace': '/DeviceGray',\n",
       "     '/Matte': [0, 0, 0],\n",
       "     '/BitsPerComponent': 8,\n",
       "     '/Interpolate': False,\n",
       "     '/Filter': '/FlateDecode'}},\n",
       "   '/Image38': {'/Type': '/XObject',\n",
       "    '/Subtype': '/Image',\n",
       "    '/Width': 320,\n",
       "    '/Height': 175,\n",
       "    '/ColorSpace': '/DeviceRGB',\n",
       "    '/BitsPerComponent': 8,\n",
       "    '/Filter': '/DCTDecode',\n",
       "    '/Interpolate': True,\n",
       "    '/SMask': {'/Type': '/XObject',\n",
       "     '/Subtype': '/Image',\n",
       "     '/Width': 320,\n",
       "     '/Height': 175,\n",
       "     '/ColorSpace': '/DeviceGray',\n",
       "     '/Matte': [0, 0, 0],\n",
       "     '/BitsPerComponent': 8,\n",
       "     '/Interpolate': False,\n",
       "     '/Filter': '/FlateDecode'}}},\n",
       "  '/Font': {'/F1': {'/Type': '/Font',\n",
       "    '/Subtype': '/TrueType',\n",
       "    '/Name': '/F1',\n",
       "    '/BaseFont': '/Times#20New#20Roman,Bold',\n",
       "    '/Encoding': '/WinAnsiEncoding',\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/Times#20New#20Roman,Bold',\n",
       "     '/Flags': 32,\n",
       "     '/ItalicAngle': 0,\n",
       "     '/Ascent': 891,\n",
       "     '/Descent': -216,\n",
       "     '/CapHeight': 677,\n",
       "     '/AvgWidth': 427,\n",
       "     '/MaxWidth': 2558,\n",
       "     '/FontWeight': 700,\n",
       "     '/XHeight': 250,\n",
       "     '/Leading': 42,\n",
       "     '/StemV': 42,\n",
       "     '/FontBBox': [-558, -216, 2000, 677]},\n",
       "    '/FirstChar': 32,\n",
       "    '/LastChar': 121,\n",
       "    '/Widths': [250,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     333,\n",
       "     333,\n",
       "     0,\n",
       "     0,\n",
       "     250,\n",
       "     333,\n",
       "     250,\n",
       "     0,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     333,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     722,\n",
       "     667,\n",
       "     722,\n",
       "     722,\n",
       "     667,\n",
       "     611,\n",
       "     778,\n",
       "     778,\n",
       "     389,\n",
       "     500,\n",
       "     778,\n",
       "     667,\n",
       "     944,\n",
       "     722,\n",
       "     778,\n",
       "     611,\n",
       "     0,\n",
       "     722,\n",
       "     556,\n",
       "     667,\n",
       "     722,\n",
       "     722,\n",
       "     1000,\n",
       "     722,\n",
       "     722,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     500,\n",
       "     556,\n",
       "     444,\n",
       "     556,\n",
       "     444,\n",
       "     333,\n",
       "     500,\n",
       "     556,\n",
       "     278,\n",
       "     0,\n",
       "     0,\n",
       "     278,\n",
       "     833,\n",
       "     556,\n",
       "     500,\n",
       "     556,\n",
       "     556,\n",
       "     444,\n",
       "     389,\n",
       "     333,\n",
       "     556,\n",
       "     500,\n",
       "     0,\n",
       "     0,\n",
       "     500]},\n",
       "   '/F2': {'/Type': '/Font',\n",
       "    '/Subtype': '/TrueType',\n",
       "    '/Name': '/F2',\n",
       "    '/BaseFont': '/Times#20New#20Roman',\n",
       "    '/Encoding': '/WinAnsiEncoding',\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/Times#20New#20Roman',\n",
       "     '/Flags': 32,\n",
       "     '/ItalicAngle': 0,\n",
       "     '/Ascent': 891,\n",
       "     '/Descent': -216,\n",
       "     '/CapHeight': 693,\n",
       "     '/AvgWidth': 401,\n",
       "     '/MaxWidth': 2568,\n",
       "     '/FontWeight': 400,\n",
       "     '/XHeight': 250,\n",
       "     '/Leading': 42,\n",
       "     '/StemV': 40,\n",
       "     '/FontBBox': [-568, -216, 2000, 693]},\n",
       "    '/FirstChar': 32,\n",
       "    '/LastChar': 239,\n",
       "    '/Widths': [250,\n",
       "     0,\n",
       "     408,\n",
       "     0,\n",
       "     0,\n",
       "     833,\n",
       "     778,\n",
       "     180,\n",
       "     333,\n",
       "     333,\n",
       "     0,\n",
       "     0,\n",
       "     250,\n",
       "     333,\n",
       "     250,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     278,\n",
       "     278,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     921,\n",
       "     722,\n",
       "     667,\n",
       "     667,\n",
       "     722,\n",
       "     611,\n",
       "     556,\n",
       "     722,\n",
       "     0,\n",
       "     333,\n",
       "     0,\n",
       "     722,\n",
       "     611,\n",
       "     889,\n",
       "     722,\n",
       "     722,\n",
       "     556,\n",
       "     0,\n",
       "     667,\n",
       "     556,\n",
       "     611,\n",
       "     722,\n",
       "     722,\n",
       "     944,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     444,\n",
       "     500,\n",
       "     444,\n",
       "     500,\n",
       "     444,\n",
       "     333,\n",
       "     500,\n",
       "     500,\n",
       "     278,\n",
       "     278,\n",
       "     500,\n",
       "     278,\n",
       "     778,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     333,\n",
       "     389,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     722,\n",
       "     500,\n",
       "     500,\n",
       "     444,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     278]},\n",
       "   '/F3': {'/Type': '/Font',\n",
       "    '/Subtype': '/TrueType',\n",
       "    '/Name': '/F3',\n",
       "    '/BaseFont': '/Times#20New#20Roman,Italic',\n",
       "    '/Encoding': '/WinAnsiEncoding',\n",
       "    '/FontDescriptor': {'/Type': '/FontDescriptor',\n",
       "     '/FontName': '/Times#20New#20Roman,Italic',\n",
       "     '/Flags': 32,\n",
       "     '/ItalicAngle': -16.4,\n",
       "     '/Ascent': 891,\n",
       "     '/Descent': -216,\n",
       "     '/CapHeight': 694,\n",
       "     '/AvgWidth': 402,\n",
       "     '/MaxWidth': 1831,\n",
       "     '/FontWeight': 400,\n",
       "     '/XHeight': 250,\n",
       "     '/Leading': 42,\n",
       "     '/StemV': 40,\n",
       "     '/FontBBox': [-498, -216, 1333, 694]},\n",
       "    '/FirstChar': 32,\n",
       "    '/LastChar': 239,\n",
       "    '/Widths': [250,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     778,\n",
       "     0,\n",
       "     333,\n",
       "     333,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     333,\n",
       "     250,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     333,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     611,\n",
       "     611,\n",
       "     667,\n",
       "     722,\n",
       "     611,\n",
       "     611,\n",
       "     722,\n",
       "     722,\n",
       "     333,\n",
       "     0,\n",
       "     667,\n",
       "     556,\n",
       "     833,\n",
       "     667,\n",
       "     722,\n",
       "     611,\n",
       "     0,\n",
       "     611,\n",
       "     500,\n",
       "     556,\n",
       "     0,\n",
       "     611,\n",
       "     833,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     500,\n",
       "     500,\n",
       "     444,\n",
       "     500,\n",
       "     444,\n",
       "     278,\n",
       "     500,\n",
       "     500,\n",
       "     278,\n",
       "     278,\n",
       "     444,\n",
       "     278,\n",
       "     722,\n",
       "     500,\n",
       "     500,\n",
       "     500,\n",
       "     0,\n",
       "     389,\n",
       "     389,\n",
       "     278,\n",
       "     500,\n",
       "     444,\n",
       "     0,\n",
       "     0,\n",
       "     444,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     760,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     0,\n",
       "     278]},\n",
       "   '/F7': {'/Type': '/Font',\n",
       "    '/Subtype': '/Type0',\n",
       "    '/BaseFont': '/Times#20New#20Roman',\n",
       "    '/Encoding': '/Identity-H',\n",
       "    '/DescendantFonts': [IndirectObject(26, 0, 2725724829584)],\n",
       "    '/ToUnicode': {'/Filter': '/FlateDecode'}}},\n",
       "  '/ProcSet': ['/PDF', '/Text', '/ImageB', '/ImageC', '/ImageI']},\n",
       " '/MediaBox': [0, 0, 595.32, 841.92],\n",
       " '/Contents': {'/Filter': '/FlateDecode'},\n",
       " '/Group': {'/Type': '/Group', '/S': '/Transparency', '/CS': '/DeviceRGB'},\n",
       " '/Tabs': '/S',\n",
       " '/StructParents': 2}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page3 = pdf_reader.getPage(2)\n",
    "page3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "af115107",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'International Journal of Recent Technology and Engineering (IJRTE)  \\nISSN: 2277 -3878, Volume -8, Issue -2S11, September 2019  \\n2425  Published By:  \\nBlue Eyes Intelligence Engineering \\n& Sciences Publication  Retrieval Number: B1280 0982S1119/2019©BEIESP  \\nDOI: 10.35940/ijrte.B1280 .0982S1119  \\n \\n voting.) It implements \"hard\" and \"soft\" voting. In hard \\nvoting,  the most frequent pre diction done by classification \\nmodels is considered as final class label. In soft voting, the \\naveraging of class -probabilities is taken as output(only \\nrecommended if the classifiers are well -calibrated).  \\n4.2  Dataset  \\nModel is trained by giving a complete da ta on which \\nsupervised learning can be done. To achieve this, user \\nmessage data has to be collected and mark them as either \\nspam or ham. The data set used is given by the UCI Machine \\nLearning Repository. It has over 5000 SMS messages which \\nare labelled and  are collected for message spam research.  \\nThe below image is a screenshot of the dataset that has \\nbeen collected for Spam research which contains all the \\nmobile messages. These messages are tagged accordingly as \\nlegitimate(ham) or spam. There are 5,574 ro ws with two \\ncolumns in the dataset.  \\n \\n \\nV. EXPERIMENTS  \\nVarious experiments are applied on the dataset which were \\nbased on Natural Language Processing(NLP) concepts like \\nlabel encoding, tokenization, stemming, stop word removal, \\ngenerating features and then applied ensemble method – \\nvoting classifier. All these experiments performed in the \\nmodel classify the data set accurately.  \\n5.1  Pre -Processing  \\nThe messages have to be pre -processed for the removal of \\nunwanted punctuation, grammar, stop words etc.  \\n5.1.1 Lab el Encoding  \\nLabel Encoder encode labels with values between „0‟ and \\n„n-1‟ where n represents the number of distinct labels for the \\nclasses. Same value is as assigned to the labels which are \\nrepeated earlier. In our experiment, we convert the class \\nlabels t o binary values, where „0‟ is ham and „1‟ is spam.  \\n5.1.2 Stop Word Removal  \\nWhen using Natural Language Processing(NLP), our goal \\nis to perform some analysis or processing so that a computer \\ncan respond to text appropriately.  A machine cannot understand the  human readable form. \\nSo, data has to be pre -processed in order to make it \\nmachine -readable. This is “pre -processing” of which one of \\nthe major forms is to filter out useless data. This useless data \\n(words) is generally referred as „stop words‟ in Natural \\nLanguage Processing(NLP).  \\n5.1.3  Stemming  \\nStemming is another pre -processing step that normalize \\nsentences. Stemming is a way to account for the variations of \\nwords and sentences which often have a same meaning; \\nfurthermore, it will help us shorten the sent ences and shorten \\nour lookup. For example, consider the following sentence:  \\n \\n1. I was taking a ride on my horse.  \\n2. I was riding my horse.  \\n \\nThese sentences mean the same thing, as noted by the same \\ntense  in  each sentence; however, that isn\\'t intuitively \\nunderstood by the computer. To account for all the variations \\nof words in the English language, we can use the Porter \\nstemmer, which has been around since 1979.  \\n5.2  Feature Generation  \\nFeature engineering is t he process of constructing features \\nfor machine learning algorithms by using the knowledge of \\nthat specific domain. The words in each text message are the \\nfeatures on which the algorithm will predict the output. For \\nthis purpose, it will be necessary to to kenize each word. The \\nmost common 1500 words that are generated in feature \\ngeneration will be used as our features. Then the data is split \\nin training and testing datasets with a test size of 25%.  \\n5.3  Implementation of Algorithms  \\nWe need to import each alg orithm from scikit -learn library \\nalong with performance metrics. We require accuracy score \\nand classification report metrics to predict the accuracy and \\ngive a classified report on the output.  \\nVI. RESULT ANALYSIS  \\nIn this part, we compare the performance an d accuracy of \\none algorithm with other machine learning algorithms. The \\nalgorithms applied in this work gave out high accuracy \\nvalues. But among the various experiments done, the spam \\nmessage is best predicted by Support Vector Classifier with \\nan accuracy of 98.49%. Other algorithms have similar \\naccuracy with a variation of about 3%. The below picture \\nrepresents the accuracy values of various algorithms applied \\non spam dataset.  \\n \\n  \\n'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F_page3 = page3.extract_text(0)\n",
    "F_page3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "46c67384",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nSpam Detection using NLP Techniques  \\n2426  Published By:  \\nBlue Eyes Intelligence Engineering \\n& Sciences Publication  Retrieval Number: B1280 0982S1119/2019©BEIESP  \\nDOI: 10.35940/ijrte.B1280 .0982S1119  \\n \\n The ensemble vote classifier applied on above algorithms \\ngave an accuracy of  98.6%. The classification report below \\ngives a detailed description of precision values.  \\nOur model predicted legitimate messages as ham 1198 \\ntimes but failed 17 times and it correctly predicted spam 176 \\ntimes but failed twice. This is represented by the \\nclassification report below.  \\n \\nVII. CONCLUSION AND FURTHER WORK  \\nThe previously collected mails are taken as dataset and for \\neach input in the set,  a class is predicted and given as output. \\nThe messages are first tagged correctly to apply algorithms \\non them. Applying various classifiers helps us to know the \\nbest and the worst algorithms for a problem.  \\nThe SVM algorithm was very effective outputting a  high \\nsuccess percentage, up to 98%. This confirms that SVM is \\none of the best model for filtering spam messages in the \\ninbox. This model need to be improved to understand \\nsarcasm, context on the whole which could be essential while \\ndetecting spam.  \\nVIII. ACKNOWLEDGMENTS  \\nI would like to thank Dr.M.RamaBai, Professor, Dept. of \\nCSE, for extending her help, support and guidance during this \\nwork . \\nREFERENCES  \\n1. Navaney, P., Dubey, G., &Rana, A. (2018). \"SMS   Spam \\nFiltering Using Supervised Machine Learning \\nAlgorithms.\" 2018 8th International Conference on Cloud \\nComputing, Data Science & Engineering (Confluence).  \\n2. Mathew, K., &Issac, B. (2011). \"Intelligent spam \\nclassification for mobile text message.\" Proceedings of \\n2011 International Conference on Computer S cience and \\nNetwork Technology . \\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "page4 = pdf_reader.getPage(3)\n",
    "page4\n",
    "F_page4 = page4.extract_text(0)\n",
    "F_page4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "662d7776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[\\'International Journal of Recent Technology and Engineering (IJRTE)  \\\\nISSN: 2277 -3878, Volume -8, Issue -2S11, September 2019  \\\\n2423  Published By:  \\\\nBlue Eyes Intelligence Engineering \\\\n& Sciences Publication  Retrieval Number: B1280 0982S1119/2019©BEIESP  \\\\nDOI: 10.35940/ijrte.B1280 .0982S1119  \\\\n \\\\n \\\\uf020 \\\\nAbstract —Natural Language Processing is a vital field of \\\\nresearch having applications in different subjects. Text \\\\nClassification is a part of NLP where the text is converted into a \\\\nmachine -readable form by performing various methods. \\\\nTokenizing, part -of-speech tagging, stemming, chunking are some \\\\nof the text classification methods. Implementing these methods on \\\\nour data gives us a classified data on which we will train the model \\\\nto detect spam and ham messages using Scikit -Learn Classifiers. \\\\nWe proposed a m odel to solve the issue of classifying messages as \\\\nspam or ham by experimenting and analyzing the relative \\\\nstrengths of several machine learning algorithms such as \\\\nK-Nearest Neighbors (KNN), Decision Tree Classifier, Random \\\\nForest Classifier, Logistic Regr ession, SGD Classifier, \\\\nMultinomial Naive Bayes(NB), Support Vector Machine(SVM) to \\\\nhave a logical comparison of the performance measures of the \\\\nmethods we utilized in this research. The algorithm we proposed \\\\nachieved an average accuracy of 98.49% with SVM  model on \\\\n‘SMS Spam Collection’ dataset . \\\\nI. INTRODUCTION  \\\\nWe get hundreds of messages from unknown sources and \\\\nour inbox is filled with unwanted emails. These unwanted \\\\nmessages are called spam and essential messages are called \\\\nham mails. We will prepare a m odel that will categorize \\\\nmessages in mobile devices as spam or ham. In order to \\\\nachieve this, data from the messages is to be collected first \\\\nand natural language processing techniques are to be applied \\\\non it.  \\\\nThe spam filtering among messages helps the m obile user \\\\nto have a good visualization of the inbox. Unnecessary \\\\nmessages will be marked as spam so users need not waste \\\\ntheir time reading them. In this paper, we propose to classify \\\\ndata in the messages as either spam (unwanted) or \\\\nham(wanted) messages.  We devised our own spam detector.  \\\\nII. RELATED WORK  \\\\nIdentifying spam messages from inbox has been done by \\\\nvarious methodologies. Below are some of the approaches.  \\\\n1. Sethi, P., Bhandari, V., &Kohli, B. (2017). “SMS spam \\\\ndetection and comparison of various machine learning \\\\nalgorithms.” 2017 International Conference on Computing \\\\nand Communication Technologies for Smart Nation \\\\n(IC3TSN).  \\\\n2. DelviaArifin, D., Shaufiah, &Bijaksana, M. A. \\\\n(2016).“Enhancing spam detection on mobile phone Short \\\\nMessage Service (SMS)  performance using FP -growth and \\\\nNaive Bayes Classifier.”2016 IEEE Asia Pacific Conference \\\\non Wireless and Mobile (APWiMob).  \\\\n3. Agarwal, S., Kaur, S., &Garhwal, S. (2015). “SMS spam \\\\ndetection for Indian messages.” 2015 1st International \\\\nConference on Next Generation Computing Technologies  \\\\n \\\\nRevised Versi on Manuscript Received on 10 September, 2019 . \\\\nBollamPragna , (email: bpragna98@gmail.com)  \\\\nDr.M.RamaBai (Professor), (email: rama@mgit.ac.in)   \\\\n(NGCT).  \\\\n4.   Gupta, M., Bakliwal, A., Agarwal, S., &Mehndiratta, \\\\nP. (2018). “A Comparative Study of Spam SMS Detection \\\\nUsing Machine Learning Classifiers.” 2018 Eleventh \\\\nInternational Conference on Contemporary Computing  \\\\n(IC3).  \\\\nIII. ALGORITHM  \\\\nSpam is unsolicited bulk messages that are not required for \\\\nthe users but are forced into their inbox. Spams are sent \\\\nmostly by advertisers, tricksters or by fraud people. \\\\nUnderstanding if a message is spam or not can be easily done  \\\\nby reading it once. Our purpose is to detect spam by using \\\\nvarious algorithms and measuring their accuracy to find the \\\\nbest fitting algorithm. The common classical approaches \\\\nwhich use white -lists and black -lists methods do not work \\\\nproperly as they are o nly capable of blocking an entire server \\\\n(source) from sending messages, that may contain some \\\\nimportant messages (false positives). Therefore, spam \\\\nfiltration is to be done using text classification techniques.  \\\\nIn our experiments we performed text pre -pro cessing in \\\\nthe first part which gives the annotated data that is split into \\\\ntwo parts called training set and testing set to check the \\\\naccuracy. The below figure represents the flow of our \\\\ntraining and testing on the data using various algorithms.  \\\\n \\\\n \\\\nVarious types of spam filters used are given below.  \\\\n1. Blatant Blocking -  \\\\n  \\\\nSpam Detection using NLP Techniques  \\\\nBollamPragna, M.RamaBai   \\\\nSpam Detection using NLP Techniques  \\\\n2424  Published By:  \\\\nBlue Eyes Intelligence Engineering \\\\n& Sciences Publication  Retrieval Number: B1280 0982S1119/2019©BEIESP  \\\\nDOI: 10.35940/ijrte.B1280 .0982S1119  \\\\n \\\\n Emails are deleted even before they reach the inbox. This \\\\nblocking is done blatantly.  \\\\n2. Bulk Email Filter -  \\\\nThis filters out those emails which are passed on through \\\\nother catego ries but are unnecessary or spam messages.  \\\\n3. Category Filters -  \\\\nAccording to the specific content like email addresses etc, \\\\na user is allowed to define their own rule to enable the \\\\nfiltering of the messages. There can be one or more \\\\nuser -defined rules.  \\\\n4. Null Sender Disposition -  \\\\nMessages are disposed if they do not have an SMTP \\\\nenvelope sender address.  \\\\n5. Null Sender Header Tag Validation -  \\\\nAll the messages are validated by checking security digital \\\\nsignature of each message in the inbox.  \\\\nIV. FRAMEWORK  \\\\nVarious tools, techniques and data set used in this work is \\\\ndescribed in this section. As cellular messages repeatedly \\\\nhave a number of acronyms, efficiency of the filters is \\\\naffected by it. So a large and valid message dataset is used in \\\\nthis process.  \\\\n4.1  Tools and Algorithms  \\\\nThe machine learning algorithms used in the work are \\\\ndescribed in detail in this section.  \\\\n4.1.1 Naïve Bayes (NB)  \\\\nNaive Bayes Classiﬁer uses Bayes Theorem, which \\\\ndetermines the occurrence probability of an event \\\\nconsidering the probability of an occurred event. Linearly \\\\nseparable problems are solved extremely well by Naïve  \\\\nBayes classiﬁer and for non -linearly separab le questions, it \\\\nperforms reasonably good.  \\\\nMultinomial Naive Bayes classifier uses a multinomial \\\\ndistribution for each one of the features generated on data. \\\\nThis is a particular instance of a Naive Bayes classifier.  \\\\n4.1.2 Stochastic Gradient Descent  \\\\nAn o bjective function is optimized iteratively with suitable \\\\nsmoothness properties (e.g. subdifferentiable or \\\\ndifferentiable) in Stochastic Gradient Descent Algorithm \\\\n(SGD).  \\\\n4.1.3 Support Vector Machine (SVM)  \\\\nThe SVM classiﬁer creates an N -dimensional hyperpl ane \\\\nwhich divides the data into two categories. SVM models are \\\\nsimilar to a Neural Network. SVM classifier usually takes the \\\\ninput data and for every input taken it outputs the class to \\\\nwhich this input belongs. Two class problems are solved by \\\\nSVM which i s a non -probabilistic binary linear classiﬁer.  \\\\n4.1.4 Logistic Regression  \\\\nAlthough many sophisticated statistical models exit, \\\\nLogistic regression( or logit regression) uses a logistic \\\\nfunction to model a binary dependent variable. In regression \\\\nanalysis, i t estimates the parameters of a logit model which is \\\\nin the form of binary regression. In mathematical words a binary logistic model has a dependent variable with two \\\\npossible outcomes. These outcomes can be labelled as “0” \\\\nand “1” which usually represent two opposite classes such as \\\\npass/fail, win/loose.  \\\\n4.1.5  K-Nearest Neighbours  \\\\nIn pattern recognition, the k -nearest neighbors algorithm \\\\n(k-NN) is a non -parametric method used for classification \\\\nand regression. In both cases, the input is same but the outpu t \\\\nvaries. Input contains the  closest k training examples in the \\\\nfeature space whereas, the output is decided depending on the \\\\nk-NN usage for classification or regression:  \\\\n1. The output of a k -NN classifier has a class membership. \\\\nClassification of object s is done by a plurality vote of its \\\\nneighbours and the object is assigned to the class that is most \\\\ncommon among its k nearest neighbours. When „k‟ has a \\\\nvalue one, the object is directly given to the class which has a \\\\nsingle nearest neighbour.  \\\\n2. In k -NN  regression, property value gives the output for \\\\nthe object. This value is the average of the values of k nearest \\\\nneighbours.  \\\\n \\\\n \\\\n4.1.6  Random Forest Classifier  \\\\nEnsemble learning method for classification, regression \\\\nand other tasks is Random forests(or rand om decision \\\\nforests) which operate at training time by constructing a \\\\nmultitude of decision trees and giving the class as output \\\\nwhich is the mean prediction of the individual trees or mode \\\\nof the classes.  \\\\n4.1.7 Decision Tree Classifier  \\\\nA decision tree has  a flowchart -like structure containing \\\\nnodes. Each internal node is a \"test\" on an attribute which \\\\nbranches into two nodes. These two nodes are the outputs of \\\\nthe decision in the test. The tree ends with leaf nodes which \\\\nrepresent a class label (decision t aken after computing all \\\\nattributes). The path from the root node to reach one leaf node \\\\nis a single classification rule. Three types of nodes are in a \\\\ndecision tree which are given below.  \\\\n \\\\n1. Decision nodes – generally given by squares  \\\\n2. Chance nodes – generally given by circles  \\\\n3. End nodes – generally given by triangles.  \\\\n4.1.8 Ensemble Methods  \\\\nThe meta -algorithms combine various machine learning \\\\nmethods into one model which predicts the output and either \\\\nreduces the variance (bagging), bias (boosting),  or enhance \\\\npredictions (stacking). We used the voting classifier in our \\\\nexperiment.  \\\\nVoting Classifier  \\\\nThe Ensemble Vote Classifier is a meta -classifier that \\\\ncombines machine learning classifiers for classification by  \\\\nreferring  to majority or plurality voting which are either \\\\nsimilar or conceptually different. (For simplicity, we will \\\\nrefer to both majority and plurality voting as majority  \\\\n  International Journal of Recent Technology and Engineering (IJRTE)  \\\\nISSN: 2277 -3878, Volume -8, Issue -2S11, September 2019  \\\\n2425  Published By:  \\\\nBlue Eyes Intelligence Engineering \\\\n& Sciences Publication  Retrieval Number: B1280 0982S1119/2019©BEIESP  \\\\nDOI: 10.35940/ijrte.B1280 .0982S1119  \\\\n \\\\n voting.) It implements \"hard\" and \"soft\" voting. In hard \\\\nvoting,  the most frequent pre diction done by classification \\\\nmodels is considered as final class label. In soft voting, the \\\\naveraging of class -probabilities is taken as output(only \\\\nrecommended if the classifiers are well -calibrated).  \\\\n4.2  Dataset  \\\\nModel is trained by giving a complete da ta on which \\\\nsupervised learning can be done. To achieve this, user \\\\nmessage data has to be collected and mark them as either \\\\nspam or ham. The data set used is given by the UCI Machine \\\\nLearning Repository. It has over 5000 SMS messages which \\\\nare labelled and  are collected for message spam research.  \\\\nThe below image is a screenshot of the dataset that has \\\\nbeen collected for Spam research which contains all the \\\\nmobile messages. These messages are tagged accordingly as \\\\nlegitimate(ham) or spam. There are 5,574 ro ws with two \\\\ncolumns in the dataset.  \\\\n \\\\n \\\\nV. EXPERIMENTS  \\\\nVarious experiments are applied on the dataset which were \\\\nbased on Natural Language Processing(NLP) concepts like \\\\nlabel encoding, tokenization, stemming, stop word removal, \\\\ngenerating features and then applied ensemble method – \\\\nvoting classifier. All these experiments performed in the \\\\nmodel classify the data set accurately.  \\\\n5.1  Pre -Processing  \\\\nThe messages have to be pre -processed for the removal of \\\\nunwanted punctuation, grammar, stop words etc.  \\\\n5.1.1 Lab el Encoding  \\\\nLabel Encoder encode labels with values between „0‟ and \\\\n„n-1‟ where n represents the number of distinct labels for the \\\\nclasses. Same value is as assigned to the labels which are \\\\nrepeated earlier. In our experiment, we convert the class \\\\nlabels t o binary values, where „0‟ is ham and „1‟ is spam.  \\\\n5.1.2 Stop Word Removal  \\\\nWhen using Natural Language Processing(NLP), our goal \\\\nis to perform some analysis or processing so that a computer \\\\ncan respond to text appropriately.  A machine cannot understand the  human readable form. \\\\nSo, data has to be pre -processed in order to make it \\\\nmachine -readable. This is “pre -processing” of which one of \\\\nthe major forms is to filter out useless data. This useless data \\\\n(words) is generally referred as „stop words‟ in Natural \\\\nLanguage Processing(NLP).  \\\\n5.1.3  Stemming  \\\\nStemming is another pre -processing step that normalize \\\\nsentences. Stemming is a way to account for the variations of \\\\nwords and sentences which often have a same meaning; \\\\nfurthermore, it will help us shorten the sent ences and shorten \\\\nour lookup. For example, consider the following sentence:  \\\\n \\\\n1. I was taking a ride on my horse.  \\\\n2. I was riding my horse.  \\\\n \\\\nThese sentences mean the same thing, as noted by the same \\\\ntense  in  each sentence; however, that isn\\\\\\'t intuitively \\\\nunderstood by the computer. To account for all the variations \\\\nof words in the English language, we can use the Porter \\\\nstemmer, which has been around since 1979.  \\\\n5.2  Feature Generation  \\\\nFeature engineering is t he process of constructing features \\\\nfor machine learning algorithms by using the knowledge of \\\\nthat specific domain. The words in each text message are the \\\\nfeatures on which the algorithm will predict the output. For \\\\nthis purpose, it will be necessary to to kenize each word. The \\\\nmost common 1500 words that are generated in feature \\\\ngeneration will be used as our features. Then the data is split \\\\nin training and testing datasets with a test size of 25%.  \\\\n5.3  Implementation of Algorithms  \\\\nWe need to import each alg orithm from scikit -learn library \\\\nalong with performance metrics. We require accuracy score \\\\nand classification report metrics to predict the accuracy and \\\\ngive a classified report on the output.  \\\\nVI. RESULT ANALYSIS  \\\\nIn this part, we compare the performance an d accuracy of \\\\none algorithm with other machine learning algorithms. The \\\\nalgorithms applied in this work gave out high accuracy \\\\nvalues. But among the various experiments done, the spam \\\\nmessage is best predicted by Support Vector Classifier with \\\\nan accuracy of 98.49%. Other algorithms have similar \\\\naccuracy with a variation of about 3%. The below picture \\\\nrepresents the accuracy values of various algorithms applied \\\\non spam dataset.  \\\\n \\\\n  \\\\n \\\\nSpam Detection using NLP Techniques  \\\\n2426  Published By:  \\\\nBlue Eyes Intelligence Engineering \\\\n& Sciences Publication  Retrieval Number: B1280 0982S1119/2019©BEIESP  \\\\nDOI: 10.35940/ijrte.B1280 .0982S1119  \\\\n \\\\n The ensemble vote classifier applied on above algorithms \\\\ngave an accuracy of  98.6%. The classification report below \\\\ngives a detailed description of precision values.  \\\\nOur model predicted legitimate messages as ham 1198 \\\\ntimes but failed 17 times and it correctly predicted spam 176 \\\\ntimes but failed twice. This is represented by the \\\\nclassification report below.  \\\\n \\\\nVII. CONCLUSION AND FURTHER WORK  \\\\nThe previously collected mails are taken as dataset and for \\\\neach input in the set,  a class is predicted and given as output. \\\\nThe messages are first tagged correctly to apply algorithms \\\\non them. Applying various classifiers helps us to know the \\\\nbest and the worst algorithms for a problem.  \\\\nThe SVM algorithm was very effective outputting a  high \\\\nsuccess percentage, up to 98%. This confirms that SVM is \\\\none of the best model for filtering spam messages in the \\\\ninbox. This model need to be improved to understand \\\\nsarcasm, context on the whole which could be essential while \\\\ndetecting spam.  \\\\nVIII. ACKNOWLEDGMENTS  \\\\nI would like to thank Dr.M.RamaBai, Professor, Dept. of \\\\nCSE, for extending her help, support and guidance during this \\\\nwork . \\\\nREFERENCES  \\\\n1. Navaney, P., Dubey, G., &Rana, A. (2018). \"SMS   Spam \\\\nFiltering Using Supervised Machine Learning \\\\nAlgorithms.\" 2018 8th International Conference on Cloud \\\\nComputing, Data Science & Engineering (Confluence).  \\\\n2. Mathew, K., &Issac, B. (2011). \"Intelligent spam \\\\nclassification for mobile text message.\" Proceedings of \\\\n2011 International Conference on Computer S cience and \\\\nNetwork Technology . \\\\n\\']'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = list([F_page1+F_page2+F_page3+F_page4])\n",
    "str(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e347370e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "54bfa053",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>International Journal of Recent Technology and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  International Journal of Recent Technology and..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "58270df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =df.to_csv(\"final.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34729a91",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "291ee0b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>International Journal of Recent Technology and...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  International Journal of Recent Technology and..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_csv(\"final.csv\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29690b04",
   "metadata": {},
   "source": [
    "# text_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c68fdf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c05a63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "63481ecb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stop = stopwords.words('english')\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "209dcf6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    data = sent_tokenize(text)\n",
    "    data1 = [x for x in data if x.isalpha()]\n",
    "    data2 = [x.lower() for x in data1 ]\n",
    "    data3 = [x for x in data2 if x not in stop ]\n",
    "    data4 = [wn1.lemmatize(i) for i in data3]\n",
    "    return data4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2ee455b9",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8664/2550551566.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mpreprocessed_text\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_8664/3226023346.py\u001b[0m in \u001b[0;36mpreprocess\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mpreprocess\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msent_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m     \u001b[0mdata1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misalpha\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mdata2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata1\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mdata3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mx\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mstop\u001b[0m \u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tokenize\\__init__.py\u001b[0m in \u001b[0;36msent_tokenize\u001b[1;34m(text, language)\u001b[0m\n\u001b[0;32m    105\u001b[0m     \"\"\"\n\u001b[0;32m    106\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"tokenizers/punkt/{language}.pickle\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mtokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1274\u001b[0m         \u001b[0mGiven\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturns\u001b[0m \u001b[0ma\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0msentences\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1275\u001b[0m         \"\"\"\n\u001b[1;32m-> 1276\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentences_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1277\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1278\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mdebug_decisions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36msentences_from_text\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m         \"\"\"\n\u001b[1;32m-> 1332\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_match_potential_end_contexts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m   1330\u001b[0m         \u001b[0mfollows\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mperiod\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1331\u001b[0m         \"\"\"\n\u001b[1;32m-> 1332\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0ms\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mspan_tokenize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_match_potential_end_contexts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36mspan_tokenize\u001b[1;34m(self, text, realign_boundaries)\u001b[0m\n\u001b[0;32m   1320\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrealign_boundaries\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1321\u001b[0m             \u001b[0mslices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_realign_boundaries\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1322\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msentence\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mslices\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1323\u001b[0m             \u001b[1;32myield\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1324\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_realign_boundaries\u001b[1;34m(self, text, slices)\u001b[0m\n\u001b[0;32m   1419\u001b[0m         \"\"\"\n\u001b[0;32m   1420\u001b[0m         \u001b[0mrealign\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1421\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0msentence1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence2\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_pair_iter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mslices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1422\u001b[0m             \u001b[0msentence1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msentence1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mrealign\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msentence1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1423\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0msentence2\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_pair_iter\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m    316\u001b[0m     \u001b[0miterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    317\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 318\u001b[1;33m         \u001b[0mprev\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    319\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    320\u001b[0m         \u001b[1;32mreturn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_slices_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1393\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_slices_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m         \u001b[0mlast_break\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1395\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcontext\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_match_potential_end_contexts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1396\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_contains_sentbreak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1397\u001b[0m                 \u001b[1;32myield\u001b[0m \u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlast_break\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\nltk\\tokenize\\punkt.py\u001b[0m in \u001b[0;36m_match_potential_end_contexts\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m   1373\u001b[0m         \u001b[0mbefore_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1374\u001b[0m         \u001b[0mmatches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1375\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mmatch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mreversed\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lang_vars\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mperiod_context_re\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfinditer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1376\u001b[0m             \u001b[1;31m# Ignore matches that have already been captured by matches to the right of this match\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1377\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mmatches\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mmatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[0mbefore_start\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: expected string or bytes-like object"
     ]
    }
   ],
   "source": [
    "preprocessed_text = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a842d0e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
